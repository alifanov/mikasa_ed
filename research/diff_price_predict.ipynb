{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict price diff for BTC_ETC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/5min/BTC_ETC.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1474205100</th>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>1.445601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474205400</th>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>6.873661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474205700</th>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.183290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474206000</th>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474206300</th>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>3.065969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close    volume\n",
       "datetime                                                    \n",
       "1474205100  0.002157  0.002166  0.002157  0.002157  1.445601\n",
       "1474205400  0.002166  0.002167  0.002152  0.002166  6.873661\n",
       "1474205700  0.002161  0.002166  0.002157  0.002166  0.183290\n",
       "1474206000  0.002157  0.002166  0.002157  0.002164  0.052700\n",
       "1474206300  0.002162  0.002168  0.002162  0.002168  3.065969"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_lag = 12\n",
    "for lag in range(1, max_lag+1):\n",
    "    df['close-' + str(lag)] = df['close'] - df.shift(lag)['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close-1</th>\n",
       "      <th>close-2</th>\n",
       "      <th>close-3</th>\n",
       "      <th>close-4</th>\n",
       "      <th>close-5</th>\n",
       "      <th>close-6</th>\n",
       "      <th>close-7</th>\n",
       "      <th>close-8</th>\n",
       "      <th>close-9</th>\n",
       "      <th>close-10</th>\n",
       "      <th>close-11</th>\n",
       "      <th>close-12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1474205100</th>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>1.445601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474205400</th>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>6.873661</td>\n",
       "      <td>9.220000e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474205700</th>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.183290</td>\n",
       "      <td>-2.200000e-07</td>\n",
       "      <td>9.000000e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474206000</th>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>-1.990000e-06</td>\n",
       "      <td>-2.210000e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474206300</th>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>3.065969</td>\n",
       "      <td>3.550000e-06</td>\n",
       "      <td>1.560000e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474206600</th>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>1.528710</td>\n",
       "      <td>-9.330000e-06</td>\n",
       "      <td>-5.780000e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474206900</th>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>3.667910</td>\n",
       "      <td>9.350000e-06</td>\n",
       "      <td>2.000000e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474207200</th>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>13.054269</td>\n",
       "      <td>-1.104000e-05</td>\n",
       "      <td>-1.690000e-06</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-4.600000e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474207500</th>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>5.692200</td>\n",
       "      <td>-7.000000e-08</td>\n",
       "      <td>-1.111000e-05</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-9.750000e-06</td>\n",
       "      <td>-5.300000e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474207800</th>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>5.340402</td>\n",
       "      <td>5.500000e-07</td>\n",
       "      <td>4.800000e-07</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-8.980000e-06</td>\n",
       "      <td>-9.200000e-06</td>\n",
       "      <td>2.000000e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474208100</th>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>4.647739</td>\n",
       "      <td>8.400000e-06</td>\n",
       "      <td>8.950000e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>1.410000e-06</td>\n",
       "      <td>-5.800000e-07</td>\n",
       "      <td>-8.000000e-07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474208400</th>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.228548</td>\n",
       "      <td>-4.100000e-07</td>\n",
       "      <td>7.990000e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-2.550000e-06</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>-9.900000e-07</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474208700</th>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.111729</td>\n",
       "      <td>-4.990000e-06</td>\n",
       "      <td>-5.400000e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>1.790000e-06</td>\n",
       "      <td>-7.540000e-06</td>\n",
       "      <td>-3.990000e-06</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close     volume       close-1  \\\n",
       "datetime                                                                      \n",
       "1474205100  0.002157  0.002166  0.002157  0.002157   1.445601           NaN   \n",
       "1474205400  0.002166  0.002167  0.002152  0.002166   6.873661  9.220000e-06   \n",
       "1474205700  0.002161  0.002166  0.002157  0.002166   0.183290 -2.200000e-07   \n",
       "1474206000  0.002157  0.002166  0.002157  0.002164   0.052700 -1.990000e-06   \n",
       "1474206300  0.002162  0.002168  0.002162  0.002168   3.065969  3.550000e-06   \n",
       "1474206600  0.002164  0.002168  0.002158  0.002158   1.528710 -9.330000e-06   \n",
       "1474206900  0.002159  0.002168  0.002159  0.002168   3.667910  9.350000e-06   \n",
       "1474207200  0.002161  0.002166  0.002155  0.002157  13.054269 -1.104000e-05   \n",
       "1474207500  0.002157  0.002161  0.002155  0.002156   5.692200 -7.000000e-08   \n",
       "1474207800  0.002156  0.002161  0.002156  0.002157   5.340402  5.500000e-07   \n",
       "1474208100  0.002157  0.002168  0.002157  0.002165   4.647739  8.400000e-06   \n",
       "1474208400  0.002157  0.002165  0.002157  0.002165   0.228548 -4.100000e-07   \n",
       "1474208700  0.002160  0.002160  0.002160  0.002160   0.111729 -4.990000e-06   \n",
       "\n",
       "                 close-2   close-3   close-4   close-5   close-6  \\\n",
       "datetime                                                           \n",
       "1474205100           NaN       NaN       NaN       NaN       NaN   \n",
       "1474205400           NaN       NaN       NaN       NaN       NaN   \n",
       "1474205700  9.000000e-06       NaN       NaN       NaN       NaN   \n",
       "1474206000 -2.210000e-06  0.000007       NaN       NaN       NaN   \n",
       "1474206300  1.560000e-06  0.000001  0.000011       NaN       NaN   \n",
       "1474206600 -5.780000e-06 -0.000008 -0.000008  0.000001       NaN   \n",
       "1474206900  2.000000e-08  0.000004  0.000002  0.000001  0.000011   \n",
       "1474207200 -1.690000e-06 -0.000011 -0.000007 -0.000009 -0.000010   \n",
       "1474207500 -1.111000e-05 -0.000002 -0.000011 -0.000008 -0.000010   \n",
       "1474207800  4.800000e-07 -0.000011 -0.000001 -0.000011 -0.000007   \n",
       "1474208100  8.950000e-06  0.000009 -0.000002  0.000007 -0.000002   \n",
       "1474208400  7.990000e-06  0.000009  0.000008 -0.000003  0.000007   \n",
       "1474208700 -5.400000e-06  0.000003  0.000004  0.000003 -0.000008   \n",
       "\n",
       "                 close-7       close-8       close-9  close-10  close-11  \\\n",
       "datetime                                                                   \n",
       "1474205100           NaN           NaN           NaN       NaN       NaN   \n",
       "1474205400           NaN           NaN           NaN       NaN       NaN   \n",
       "1474205700           NaN           NaN           NaN       NaN       NaN   \n",
       "1474206000           NaN           NaN           NaN       NaN       NaN   \n",
       "1474206300           NaN           NaN           NaN       NaN       NaN   \n",
       "1474206600           NaN           NaN           NaN       NaN       NaN   \n",
       "1474206900           NaN           NaN           NaN       NaN       NaN   \n",
       "1474207200 -4.600000e-07           NaN           NaN       NaN       NaN   \n",
       "1474207500 -9.750000e-06 -5.300000e-07           NaN       NaN       NaN   \n",
       "1474207800 -8.980000e-06 -9.200000e-06  2.000000e-08       NaN       NaN   \n",
       "1474208100  1.410000e-06 -5.800000e-07 -8.000000e-07  0.000008       NaN   \n",
       "1474208400 -2.550000e-06  1.000000e-06 -9.900000e-07 -0.000001  0.000008   \n",
       "1474208700  1.790000e-06 -7.540000e-06 -3.990000e-06 -0.000006 -0.000006   \n",
       "\n",
       "            close-12  \n",
       "datetime              \n",
       "1474205100       NaN  \n",
       "1474205400       NaN  \n",
       "1474205700       NaN  \n",
       "1474206000       NaN  \n",
       "1474206300       NaN  \n",
       "1474206600       NaN  \n",
       "1474206900       NaN  \n",
       "1474207200       NaN  \n",
       "1474207500       NaN  \n",
       "1474207800       NaN  \n",
       "1474208100       NaN  \n",
       "1474208400       NaN  \n",
       "1474208700  0.000003  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8759137f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['up'] = df['close'] < df.shift(-1)['close']\n",
    "sns.countplot('up', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.545063\n",
       "True     0.454937\n",
       "Name: up, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['up'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "X = df[['close-1', 'close-2', 'close-3', 'close-4', 'close-5', 'close-6', 'close-6', 'close-7', 'close-8', 'close-9', 'close-10', 'close-11', 'close-12']].values\n",
    "y = df['up'].astype(int).values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 20building tree 2 of 20\n",
      "\n",
      "building tree 4 of 20building tree 3 of 20\n",
      "\n",
      "building tree 5 of 20\n",
      "building tree 6 of 20\n",
      "building tree 7 of 20\n",
      "building tree 8 of 20\n",
      "building tree 9 of 20\n",
      "building tree 10 of 20\n",
      "building tree 11 of 20\n",
      "building tree 12 of 20\n",
      "building tree 13 of 20\n",
      "building tree 14 of 20\n",
      "building tree 15 of 20\n",
      "building tree 16 of 20\n",
      "building tree 17 of 20\n",
      "building tree 18 of 20\n",
      "building tree 19 of 20\n",
      "building tree 20 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=4,\n",
       "            oob_score=False, random_state=0, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=4, random_state=0, verbose=2, n_estimators=20)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55158503634357037"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 2.04, NNZs: 13, Bias: -0.856465, T: 78831, Avg. loss: 2.734453\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.96, NNZs: 13, Bias: -0.613246, T: 157662, Avg. loss: 1.200186\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.24, NNZs: 13, Bias: 0.586501, T: 236493, Avg. loss: 1.082998\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.32, NNZs: 13, Bias: -0.408396, T: 315324, Avg. loss: 1.043440\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.57, NNZs: 13, Bias: -0.787834, T: 394155, Avg. loss: 1.015892\n",
      "Total training time: 0.07 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vampire/PycharmProjects/mikasa_ed/venv/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", verbose=2)\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54644746356128937"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(verbose=2)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55984320888990369"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54926361456787309"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(verbose=2)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54789359515926472"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5381512349202725"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52783803326102674"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56463827682003276"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57377984968128626"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(max_depth=4)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(8, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6801 - acc: 0.5568    \n",
      "Epoch 2/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6773 - acc: 0.5655    \n",
      "Epoch 3/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6767 - acc: 0.5677    \n",
      "Epoch 4/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6766 - acc: 0.5664    \n",
      "Epoch 5/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6762 - acc: 0.5651    \n",
      "Epoch 6/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6760 - acc: 0.5692    \n",
      "Epoch 7/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6757 - acc: 0.5690    \n",
      "Epoch 8/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6755 - acc: 0.5688    \n",
      "Epoch 9/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6753 - acc: 0.5689    \n",
      "Epoch 10/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6752 - acc: 0.5700    \n",
      " 9950/10512 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6800 - acc: 0.5600    \n",
      "Epoch 2/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6773 - acc: 0.5652    \n",
      "Epoch 3/10\n",
      "94596/94596 [==============================] - 9s - loss: 0.6767 - acc: 0.5651     \n",
      "Epoch 4/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6762 - acc: 0.5660    \n",
      "Epoch 5/10\n",
      "94596/94596 [==============================] - 9s - loss: 0.6759 - acc: 0.5659     \n",
      "Epoch 6/10\n",
      "94596/94596 [==============================] - 9s - loss: 0.6757 - acc: 0.5674     \n",
      "Epoch 7/10\n",
      "94596/94596 [==============================] - 9s - loss: 0.6756 - acc: 0.5668     \n",
      "Epoch 8/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6751 - acc: 0.5673    \n",
      "Epoch 9/10\n",
      "94596/94596 [==============================] - 12s - loss: 0.6752 - acc: 0.5668    \n",
      "Epoch 10/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6749 - acc: 0.5678    \n",
      "10100/10512 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "94596/94596 [==============================] - 12s - loss: 0.6801 - acc: 0.5591    \n",
      "Epoch 2/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6776 - acc: 0.5650    \n",
      "Epoch 3/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6771 - acc: 0.5642    \n",
      "Epoch 4/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6766 - acc: 0.5659    \n",
      "Epoch 5/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6762 - acc: 0.5657    \n",
      "Epoch 6/10\n",
      "94596/94596 [==============================] - 9s - loss: 0.6760 - acc: 0.5676     \n",
      "Epoch 7/10\n",
      "94596/94596 [==============================] - 9s - loss: 0.6757 - acc: 0.5681     \n",
      "Epoch 8/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6754 - acc: 0.5673    \n",
      "Epoch 9/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6753 - acc: 0.5678    \n",
      "Epoch 10/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6752 - acc: 0.5676    \n",
      " 9800/10512 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6799 - acc: 0.5575    \n",
      "Epoch 2/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6772 - acc: 0.5648    \n",
      "Epoch 3/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6769 - acc: 0.5652    \n",
      "Epoch 4/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6765 - acc: 0.5644    \n",
      "Epoch 5/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6762 - acc: 0.5656    \n",
      "Epoch 6/10\n",
      "94596/94596 [==============================] - 10s - loss: 0.6760 - acc: 0.5652    \n",
      "Epoch 7/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6759 - acc: 0.5648    \n",
      "Epoch 8/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6756 - acc: 0.5653    \n",
      "Epoch 9/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6754 - acc: 0.5652    \n",
      "Epoch 10/10\n",
      "94596/94596 [==============================] - 11s - loss: 0.6753 - acc: 0.5660    \n",
      "10200/10512 [============================>.] - ETA: 0sEpoch 1/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6805 - acc: 0.5547    \n",
      "Epoch 2/10\n",
      "94598/94598 [==============================] - 12s - loss: 0.6779 - acc: 0.5623    \n",
      "Epoch 3/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6771 - acc: 0.5640    \n",
      "Epoch 4/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6767 - acc: 0.5640    \n",
      "Epoch 5/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6765 - acc: 0.5651    \n",
      "Epoch 6/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6763 - acc: 0.5628    \n",
      "Epoch 7/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6761 - acc: 0.5640    \n",
      "Epoch 8/10\n",
      "94598/94598 [==============================] - 12s - loss: 0.6760 - acc: 0.5656    \n",
      "Epoch 9/10\n",
      "94598/94598 [==============================] - 13s - loss: 0.6756 - acc: 0.5650    \n",
      "Epoch 10/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6755 - acc: 0.5671    \n",
      "10330/10510 [============================>.] - ETA: 0sEpoch 1/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6800 - acc: 0.5596    \n",
      "Epoch 2/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6774 - acc: 0.5658    \n",
      "Epoch 3/10\n",
      "94598/94598 [==============================] - 9s - loss: 0.6769 - acc: 0.5669     \n",
      "Epoch 4/10\n",
      "94598/94598 [==============================] - 9s - loss: 0.6767 - acc: 0.5664     \n",
      "Epoch 5/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6763 - acc: 0.5676    \n",
      "Epoch 6/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6762 - acc: 0.5668    \n",
      "Epoch 7/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6759 - acc: 0.5678    \n",
      "Epoch 8/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6758 - acc: 0.5669    \n",
      "Epoch 9/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6757 - acc: 0.5678    \n",
      "Epoch 10/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6755 - acc: 0.5678    \n",
      "10450/10510 [============================>.] - ETA: 0sEpoch 1/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6801 - acc: 0.5597    \n",
      "Epoch 2/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6777 - acc: 0.5636    \n",
      "Epoch 3/10\n",
      "94598/94598 [==============================] - 9s - loss: 0.6771 - acc: 0.5641     \n",
      "Epoch 4/10\n",
      "94598/94598 [==============================] - 9s - loss: 0.6768 - acc: 0.5657     \n",
      "Epoch 5/10\n",
      "94598/94598 [==============================] - 9s - loss: 0.6766 - acc: 0.5634     \n",
      "Epoch 6/10\n",
      "94598/94598 [==============================] - 9s - loss: 0.6762 - acc: 0.5640     \n",
      "Epoch 7/10\n",
      "94598/94598 [==============================] - 9s - loss: 0.6761 - acc: 0.5652     \n",
      "Epoch 8/10\n",
      "94598/94598 [==============================] - ETA: 0s - loss: 0.6761 - acc: 0.565 - 9s - loss: 0.6762 - acc: 0.5659     \n",
      "Epoch 9/10\n",
      "94598/94598 [==============================] - 9s - loss: 0.6759 - acc: 0.5656     \n",
      "Epoch 10/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6758 - acc: 0.5653    \n",
      "10000/10510 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6800 - acc: 0.5590    \n",
      "Epoch 2/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6775 - acc: 0.5642    \n",
      "Epoch 3/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6766 - acc: 0.5659    \n",
      "Epoch 4/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6762 - acc: 0.5657    \n",
      "Epoch 5/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6758 - acc: 0.5669    \n",
      "Epoch 6/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6756 - acc: 0.5676    \n",
      "Epoch 7/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6753 - acc: 0.5687    \n",
      "Epoch 8/10\n",
      "94598/94598 [==============================] - 12s - loss: 0.6752 - acc: 0.5680    \n",
      "Epoch 9/10\n",
      "94598/94598 [==============================] - 12s - loss: 0.6748 - acc: 0.5685    \n",
      "Epoch 10/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6747 - acc: 0.5689    \n",
      "10040/10510 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "94598/94598 [==============================] - 12s - loss: 0.6797 - acc: 0.5587    \n",
      "Epoch 2/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6773 - acc: 0.5635    \n",
      "Epoch 3/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6766 - acc: 0.5647    \n",
      "Epoch 4/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6762 - acc: 0.5648    \n",
      "Epoch 5/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6759 - acc: 0.5655    \n",
      "Epoch 6/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6755 - acc: 0.5664    \n",
      "Epoch 7/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6752 - acc: 0.5668    \n",
      "Epoch 8/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6751 - acc: 0.5676    \n",
      "Epoch 9/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6749 - acc: 0.5673    \n",
      "Epoch 10/10\n",
      "94598/94598 [==============================] - 12s - loss: 0.6746 - acc: 0.5683    \n",
      " 9620/10510 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6801 - acc: 0.5582    \n",
      "Epoch 2/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6774 - acc: 0.5640    \n",
      "Epoch 3/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6769 - acc: 0.5643    \n",
      "Epoch 4/10\n",
      "94598/94598 [==============================] - 10s - loss: 0.6767 - acc: 0.5632    \n",
      "Epoch 5/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6763 - acc: 0.5666    \n",
      "Epoch 6/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6760 - acc: 0.5630    \n",
      "Epoch 7/10\n",
      "94598/94598 [==============================] - 12s - loss: 0.6760 - acc: 0.5657    \n",
      "Epoch 8/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6756 - acc: 0.5658    \n",
      "Epoch 9/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6755 - acc: 0.5677    \n",
      "Epoch 10/10\n",
      "94598/94598 [==============================] - 11s - loss: 0.6752 - acc: 0.5664    \n",
      "10050/10510 [===========================>..] - ETA: 0sResults: 56.57% (0.28%)\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, nb_epoch=100, batch_size=10, verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, X, y, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
